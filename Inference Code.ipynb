{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13655341,"sourceType":"datasetVersion","datasetId":8681357},{"sourceId":13727394,"sourceType":"datasetVersion","datasetId":8733657},{"sourceId":643069,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":484892,"modelId":500373}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install \"numpy==1.23.5\" ultralytics --no-cache-dir\nimport numpy as np","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-14T13:31:28.534Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U ultralytics","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"execution_failed":"2025-11-14T13:31:28.535Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# core\nimport os\nimport json\nimport random\nfrom typing import Iterable, Tuple, Dict, Any\n\n# numeric & plotting\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n# vision\nimport cv2 as cv\nimport torch\n\n# ultralytics YOLO\nfrom ultralytics import YOLO","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-14T13:31:28.535Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"markdown","source":"# Main","metadata":{}},{"cell_type":"markdown","source":"## Methods","metadata":{}},{"cell_type":"code","source":"def apply_clahe_bgr(frame_bgr: np.ndarray) -> np.ndarray:\n    hsv = cv.cvtColor(frame_bgr, cv.COLOR_BGR2HSV)\n    h, s, v = cv.split(hsv)\n    clahe = cv.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    v2 = clahe.apply(v)\n    hsv2 = cv.merge([h, s, v2])\n    return cv.cvtColor(hsv2, cv.COLOR_HSV2BGR)\n\ndef safe_clip_box(x1, y1, x2, y2, W, H):\n    xi1 = max(0, min(int(round(x1)), W - 1))\n    yi1 = max(0, min(int(round(y1)), H - 1))\n    xi2 = max(0, min(int(round(x2)), W - 1))\n    yi2 = max(0, min(int(round(y2)), H - 1))\n    # ensure proper order\n    if xi2 <= xi1 or yi2 <= yi1:\n        return None\n    return xi1, yi1, xi2, yi2\n\ndef l2_normalize(vec: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n    n = np.linalg.norm(vec)\n    if n < eps:\n        return vec\n    return vec / n\n\ndef cosine_similarity(a: np.ndarray, b: np.ndarray, eps: float = 1e-12) -> float:\n    # expects 1D float32 vectors\n    na = np.linalg.norm(a)\n    nb = np.linalg.norm(b)\n    if na < eps or nb < eps:\n        return 0.0\n    return float(np.dot(a, b) / (na * nb))\n\ndef image_to_vector(img_bgr: np.ndarray) -> np.ndarray:\n    # Convert to float32 and scale to [0,1], flatten\n    v = img_bgr.astype(np.float32) / 255.0\n    return v.reshape(-1)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-14T13:31:28.536Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL_PATH = '/kaggle/input/yolov11-weights-zalo-ai-challenge-2025/other/default/1/best.pt'\n\nTEST_DATA_DIR = '/kaggle/input/zalo-ai-challenge-2025-track-1-dataset/public_test/samples'\n\nOUTPUT_FILE = 'predictions.json'\n\nREF_IMG_DIR = '/kaggle/input/ref-images-zalo-ai-challenge-2025/Ref'\n\nCONFIDENCE_THRESHOLD = 0.25","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-14T13:31:28.535Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"def run_inference(\n    TTA: bool = True,\n    REF_IMG_DIR: str = REF_IMG_DIR,\n    TEST_DATA_DIR: str = TEST_DATA_DIR,\n    MODEL_PATH: str = MODEL_PATH,\n    OUTPUT_FILE: str = OUTPUT_FILE,\n    CLAHE: bool = True,\n):\n    try:\n        YOLO11_n = YOLO(MODEL_PATH)\n        print(f\"Successfully loaded model from {MODEL_PATH}\")\n    except Exception as e:\n        print(f\"Error: Could not load model from {MODEL_PATH}\")\n        print(e)\n        return\n\n    try:\n        video_folders = sorted([f for f in os.listdir(TEST_DATA_DIR) if os.path.isdir(os.path.join(TEST_DATA_DIR, f))])\n    except FileNotFoundError:\n        print(f\"Error: Test data directory not found at: {TEST_DATA_DIR}\")\n        return\n    if not video_folders:\n        print(f\"Error: No video folders found in {TEST_DATA_DIR}\")\n        return\n    print(f\"Found {len(video_folders)} videos to process...\")\n\n    ref_images = []\n    ref_cache = {}\n    for p in os.listdir(REF_IMG_DIR):\n        img = cv.imread(os.path.join(REF_IMG_DIR, p))\n        if img is None:\n            print(f\"Warning: cannot read reference image: {p}\")\n            continue\n        ref_images.append((p, img))\n    if not ref_images:\n        print(\"Warning: no valid reference images loaded; cosine similarity will be skipped.\")\n\n    all_predictions = []\n\n    for video_folder_name in video_folders:\n        video_path = os.path.join(TEST_DATA_DIR, video_folder_name, \"drone_video.mp4\")\n        if not os.path.exists(video_path):\n            print(f\"Warning: 'drone_video.mp4' not found in {video_folder_name}, skipping.\")\n            continue\n\n        video_bboxes = []\n\n        try:\n            cap = cv.VideoCapture(video_path)\n            if not cap.isOpened():\n                raise RuntimeError(f\"Cannot open video: {video_path}...\")\n            idx = 0\n            while True:\n                ok, frame = cap.read()\n                if not ok:\n                    break\n                if CLAHE:\n                    frame = apply_clahe_bgr(frame)\n                    \n                if (idx == 0):\n                    print(f'Frame Type: {type(frame)}')\n                    print(f\"Frame Shape: {frame.shape}\")\n                    \n                if frame.dtype != np.uint8: frame = (np.clip(frame, 0, 255)).astype(np.uint8)\n                results_list = YOLO11_n.predict(\n                    frame,\n                    imgsz=640,\n                    conf=CONFIDENCE_THRESHOLD,\n                    verbose=False,\n                    augment=TTA\n                )\n                if not results_list:\n                    idx += 1\n                    continue\n                results = results_list[0]\n\n                # Extract detections\n                if results.boxes is None or len(results.boxes) == 0:\n                    idx += 1\n                    continue\n\n                H, W = frame.shape[:2]\n                xyxy = results.boxes.xyxy.detach().cpu().numpy()\n                confs = results.boxes.conf.detach().cpu().numpy()\n\n                score = []\n                for i in range(xyxy.shape[0]):\n                    x1, y1, x2, y2 = xyxy[i]\n                    clipped = safe_clip_box(x1, y1, x2, y2, W, H)\n                    if clipped is None:\n                        continue\n                    xi1, yi1, xi2, yi2 = clipped\n                \n                    crop = frame[yi1:yi2, xi1:xi2]\n                    if crop.size == 0:\n                        continue\n                    base_conf = float(confs[i])\n                    best_sim = -10\n                \n                    if ref_images:\n                        ch, cw = crop.shape[:2]\n                        key = (cw, ch)\n                        if key not in ref_cache:\n                            vecs = []\n                            for ref_path, ref_img in ref_images:\n                                ref_resized = cv.resize(ref_img, (cw, ch), interpolation=cv.INTER_LINEAR)\n                                ref_vec = l2_normalize(image_to_vector(ref_resized))\n                                vecs.append((ref_path, ref_vec))\n                            ref_cache[key] = vecs\n                \n                        crop_vec = l2_normalize(image_to_vector(crop))\n                        for ref_path, ref_vec in ref_cache[key]:\n                            sim = cosine_similarity(crop_vec, ref_vec)\n                            if float(sim) > float(best_sim):\n                                best_sim = sim\n                    combined_score = best_sim * base_conf\n                    score.append(combined_score)\n\n                x1best, y1best, x2best, y2best = xyxy[np.argmax(score)]\n                bbox_data = {\n                    \"frame\": int(idx),\n                    \"x1\": int(x1best),\n                    \"y1\": int(y1best),\n                    \"x2\": int(x2best),\n                    \"y2\": int(y2best),}\n                \n                video_bboxes.append(bbox_data)\n                idx += 1\n            cap.release()\n            \n        except Exception as e:\n            print(f\"Error while processing video {video_path}: {e}\")\n            continue\n\n        detections_list = []\n        if video_bboxes:\n            detections_list.append({\"bboxes\": video_bboxes})\n        final_video_obj = {\n            \"video_id\": video_folder_name,\n            \"detections\": detections_list\n        }\n        all_predictions.append(final_video_obj)\n\n    try:\n        print(f\"\\nSaving all {len(all_predictions)} video predictions to {OUTPUT_FILE}...\")\n        with open(OUTPUT_FILE, \"w\") as f:\n            json.dump(all_predictions, f, indent=4)\n        print(\"Inference complete.\")\n    except Exception as e:\n        print(f\"Error: Could not write output JSON file: {e}\")\n\nrun_inference()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-14T13:31:28.536Z"}},"outputs":[],"execution_count":null}]}